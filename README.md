# Awesome Diffusion Samplers   [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
This repository contains a collection of resources and papers on Diffusion Samplers.

## Table of Contents
- [Blogposts \& Tutorials](#blogpost)
- [Papers](#paper)

<a name="blogpost" />

## Blogposts \& Tutorials

[Models trained with unnormalized density functions: A need for a course correction](https://d2jud02ci9yv69.cloudfront.net/2025-04-28-ebm-vs-mcmc-155/blog/ebm-vs-mcmc)  
Rishal Aggarwal, et al. ICLR 2025 Blogpost Track.

<a name="paper" />

## Papers

[Rethinking Losses for Diffusion Bridge Samplers](https://arxiv.org/abs/2506.10982)  
Sebastian Sanokowski, et al. 

[Adaptive Destruction Processes for Diffusion Samplers](https://arxiv.org/abs/2506.01541)  
Timofei Gritsaev, et al. 

[NETS: A Non-Equilibrium Transport Sampler](https://arxiv.org/abs/2410.02711)  
Michael Samuel Albergo, et al. 

[From discrete-time policies to continuous-time diffusion samplers: Asymptotic equivalences and faster training](https://arxiv.org/abs/2501.06148)  
Julius Berner, et al. [[code](https://github.com/GFNOrg/gfn-diffusion/tree/stagger)] 

[Outsourced Diffusion Sampling: Efficient Posterior Inference in Latent Spaces of Generative Models](https://arxiv.org/abs/2502.06999)  
Siddarth Venkatraman, et al. ICML 2025. [[code](https://github.com/HyperPotatoNeo/Outsourced_Diffusion_Sampling)] 

[Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching](https://arxiv.org/abs/2504.11713)  
Aaron Havens, et al. ICML 2025. [[code](https://github.com/facebookresearch/adjoint_sampling)] 

[Training Neural Samplers with Reverse Diffusive KL Divergence](https://arxiv.org/abs/2410.12456)  
Jiajun He, et al. AISTATS 2025. [[code](https://github.com/jiajunhe98/DiKL)] 

[Adaptive teachers for amortized samplers](https://arxiv.org/abs/2410.01432)  
Minsu Kim, et al. ICLR 2025. [[code](https://github.com/alstn12088/adaptive-teacher)] 

[Learned Reference-based Diffusion Sampling for multi-modal distributions](https://arxiv.org/abs/2410.19449)  
Maxence Noble, et al. ICLR 2025. [[code](https://github.com/h2o64/sde_sampler_lrds)] 

[Sequential Controlled Langevin Diffusions](https://arxiv.org/abs/2412.07081)  
Junhua Chen, et al. ICLR 2025. [[code](https://github.com/anonymous3141/SCLD)] 

[End-To-End Learning of Gaussian Mixture Priors for Diffusion Sampler](https://arxiv.org/abs/2503.00524)  
Denis Blessing, et al. ICLR 2025. 

[Underdamped Diffusion Bridges with Applications to Sampling](https://arxiv.org/abs/2503.01006)  
Denis Blessing, et al. ICLR 2025. [[code](https://github.com/DenisBless/UnderdampedDiffusionBridges)] 

[Improved off-policy training of diffusion samplers](https://arxiv.org/abs/2402.05098)  
Marcin Sendera, et al. NeurIPS 2024. [[code](https://github.com/GFNOrg/gfn-diffusion)] 

[Particle Denoising Diffusion Sampler](https://arxiv.org/abs/2402.06320)  
Angus Phillips, et al. ICML 2024. [[code](https://github.com/angusphillips/particle_denoising_diffusion_sampler)] 

[Beyond ELBOs: A Large-Scale Evaluation of Variational Methods for Sampling](https://arxiv.org/abs/2406.07423)  
Denis Blessing, et al. ICML 2024. [[code](https://github.com/DenisBless/variational_sampling_methods)] 

[Iterated Denoising Energy Matching for Sampling from Boltzmann Densities](https://arxiv.org/abs/2402.06121)  
Tara Akhound-Sadegh, et al. ICML 2024. [[code](https://github.com/jarridrb/dem)] 

[Physics-informed neural networks for sampling](https://openreview.net/forum?id=KwHPBIGkET)  
Jingtong Sun, et al. ICLR 2024 Workshop on AI4DifferentialEquations In Science. 

[Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization](https://arxiv.org/abs/2310.02679)  
Dinghuai Zhang, et al. ICLR 2024. [[code](https://github.com/zdhNarsil/Diffusion-Generative-Flow-Samplers)] 

[Transport meets Variational Inference: Controlled Monte Carlo Diffusions](https://arxiv.org/abs/2307.01050)  
Francisco Vargas, et al. ICLR 2024. [[code](https://github.com/shreyaspadhy/CMCD)] 

[Improved sampling via learned diffusions](https://arxiv.org/abs/2307.01198)  
Lorenz Richter, et al. ICLR 2024. [[code](https://github.com/juliusberner/sde_sampler)]  

[An optimal control perspective on diffusion-based generative modeling](https://arxiv.org/abs/2211.01364)  
Julius Berner, et al. TMLR 2024. [[code](https://github.com/juliusberner/sde_sampler)] 

[A Theory of Continuous Generative Flow Networks](https://arxiv.org/abs/2301.12594)  
Salem Lahlou, et al. ICML 2023. [[code](https://github.com/saleml/continuous-gfn)]  

[Denoising Diffusion Samplers](https://arxiv.org/abs/2302.13834)  
Francisco Vargas, et al. ICLR 2023. [[code](https://github.com/franciscovargas/denoising_diffusion_samplers)]  

[Path Integral Sampler: a Stochastic Control Approach for Sampling](https://arxiv.org/abs/2111.15141)  
Qinsheng Zhang, et al. ICLR 2022. [[code](https://github.com/qsh-zh/pis)]  

## Contact
If you have any suggestions or want to add your own work, please feel free to create a pull request or write to [greatdraken@gmail.com](mailto:greatdraken@gmail.com).
